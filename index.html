<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>mc-cnn by jzbontar</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">mc-cnn</h1>
      <h2 class="project-tagline">Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches</h2>
      <a href="https://github.com/jzbontar/mc-cnn" class="btn">View on GitHub</a>
      <a href="https://github.com/jzbontar/mc-cnn/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/jzbontar/mc-cnn/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="stereo-matching-by-training-a-convolutional-neural-network-to-compare-image-patches" class="anchor" href="#stereo-matching-by-training-a-convolutional-neural-network-to-compare-image-patches" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches</h1>

<p>The repository contains</p>

<ul>
<li>procedures to compute the stereo matching cost with a convolutional neural network;</li>
<li>procedures to train a convolutional neural network on the stereo matching task;</li>
<li>a basic stereo method (cross-based cost aggregation, semiglobal matching,
left-right consistency check, median filter, and bilateral filter); and</li>
</ul>

<p>A NVIDIA GPU with at least 6 GB of memory is required to run on the KITTI
data set and 12 GB to run on the Middlebury data set. We tested the code on GTX
Titan (KITTI only), K80, and GTX Titan X.  Note that the network architecture
deviates from the description in the <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Zbontar_Computing_the_Stereo_2015_CVPR_paper.html">CVPR
paper</a>;
we describe the differences in our upcoming journal paper.  Please cite the
<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Zbontar_Computing_the_Stereo_2015_CVPR_paper.html">CVPR
paper</a>
if you use code from this repository in your work.</p>

<pre><code>@InProceedings{Zbontar_2015_CVPR,
    author = {Zbontar, Jure and LeCun, Yann},
    title = {Computing the Stereo Matching Cost With a Convolutional Neural Network},
    journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2015}
}
</code></pre>

<p>The code is released under the BSD 2-Clause license.</p>

<h2>
<a id="download-trained-networks" class="anchor" href="#download-trained-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Download trained networks</h2>

<ul>
<li><a href="https://s3.amazonaws.com/mc-cnn/net_kitti_fast_-a_train_all.t7">KITTI 2012 fast</a></li>
<li><a href="https://s3.amazonaws.com/mc-cnn/net_kitti_slow_-a_train_all.t7">KITTI 2012 accurate</a></li>
<li><a href="https://s3.amazonaws.com/mc-cnn/net_kitti2015_fast_-a_train_all.t7">KITTI 2015 fast</a></li>
<li><a href="https://s3.amazonaws.com/mc-cnn/net_kitti2015_slow_-a_train_all.t7">KITTI 2015 accurate</a></li>
<li><a href="https://s3.amazonaws.com/mc-cnn/net_mb_fast_-a_train_all.t7">Middlebury fast</a></li>
<li><a href="https://s3.amazonaws.com/mc-cnn/net_mb_slow_-a_train_all.t7">Middlebury accurate</a></li>
</ul>

<h2>
<a id="compute-the-matching-cost" class="anchor" href="#compute-the-matching-cost" aria-hidden="true"><span class="octicon octicon-link"></span></a>Compute the Matching Cost</h2>

<p>Install <a href="http://torch.ch/">Torch</a>, <a href="http://opencv.org/">OpenCV 2.4</a>, and
<a href="http://www.nongnu.org/pngpp/">png++</a>.</p>

<p>Run all following commands in the same directory as this README file.</p>

<p>Compile the shared libraries:</p>

<pre><code>$ cp Makefile.proto Makefile
$ make
</code></pre>

<p>The command should produce two files: <code>libadcensus.so</code> and <code>libcv.so</code>.</p>

<p>To run the stereo algorithm on a stereo pair from the KITTI 2012 training set—</p>

<ul>
<li>Left input image<br>
<img src="samples/input/kittiL.png">
</li>
<li>Right input image<br>
<img src="samples/input/kittiR.png">
</li>
</ul>

<p>—call <code>main.lua</code> with the following arguments:</p>

<pre><code>$ ./main.lua kitti fast -a predict -net_fname net/net_kitti_fast_-a_train_tr.t7 -left samples/input/kittiL.png -right samples/input/kittiR.png -disp_max 70
Writing right.bin, 1 x 70 x 370 x 1226
Writing left.bin, 1 x 70 x 370 x 1226
Writing disp.bin, 1 x 1 x 370 x 1226
</code></pre>

<p>The first two arguments (<code>kitti fast</code>) are used to set the default
hyperparameters of the stereo method. The outputs are stored as three binary
files:</p>

<ul>
<li>
<code>left.bin</code>: The matching cost after semiglobal matching and cross-based
cost aggregation with the left image treated as the reference image.<br>
<img src="samples/output/left.png">
</li>
<li>
<code>right.bin</code>: Same as <code>left.bin</code>, but with the right image treated as the
reference image.<br>
<img src="samples/output/right.png">
</li>
<li>
<code>disp.bin</code>: The disparity map after the full stereo method.<br>
<img src="samples/output/disp.png">
</li>
</ul>

<p>Use the <code>bin2png.lua</code> script to generate the <code>.png</code> images like the ones above:</p>

<pre><code>$ luajit samples/bin2png.lua 
Writing left.png
Writing right.png
Writing disp.png
</code></pre>

<p>If you wish to use the raw convolutional neural network outputs, that is,
without applying cross-based cost aggregation and semiglobal matching, run
the following command:</p>

<pre><code>$ ./main.lua kitti fast -a predict -net_fname net/net_kitti_fast_-a_train_tr.t7 -left samples/input/kittiL.png -right samples/input/kittiR.png -disp_max 70 -sm_terminate cnn
Writing right.bin, 1 x 70 x 370 x 1226
Writing left.bin, 1 x 70 x 370 x 1226
Writing disp.bin, 1 x 1 x 370 x 1226
</code></pre>

<p>The resulting disparity maps should look like this:</p>

<ul>
<li>
<code>left.png</code><br>
<img src="samples/output/left_cnn.png">
</li>
<li>
<code>right.png</code><br>
<img src="samples/output/right_cnn.png">
</li>
</ul>

<h3>
<a id="load-the-output-binary-files" class="anchor" href="#load-the-output-binary-files" aria-hidden="true"><span class="octicon octicon-link"></span></a>Load the Output Binary Files</h3>

<p>You load the binary files by memory mapping them.  We include examples of
memory mapping for some of the more popular programming languages.</p>

<ul>
<li>
<p><strong>Lua</strong></p>

<pre><code>require 'torch'
left = torch.FloatTensor(torch.FloatStorage('../left.bin')):view(1, 70, 370, 1226)
right = torch.FloatTensor(torch.FloatStorage('../right.bin')):view(1, 70, 370, 1226)
disp = torch.FloatTensor(torch.FloatStorage('../disp.bin')):view(1, 1, 370, 1226)
</code></pre>
</li>
<li>
<p><strong>Python</strong></p>

<pre><code>import numpy as np
left = np.memmap('../left.bin', dtype=np.float32, shape=(1, 70, 370, 1226))
right = np.memmap('../right.bin', dtype=np.float32, shape=(1, 70, 370, 1226))
disp = np.memmap('../disp.bin', dtype=np.float32, shape=(1, 1, 370, 1226))
</code></pre>
</li>
<li>
<p><strong>Matlab</strong></p>

<pre><code>left = memmapfile('../left.bin', 'Format', 'single').Data;
left = reshape(left, [1 70 370 1226]);
right = memmapfile('../right.bin', 'Format', 'single').Data;
right = reshape(right, [1 70 370 1226]);
disp = memmapfile('../disp.bin', 'Format', 'single').Data;
disp = reshape(right, [1 1 370 1226]);
</code></pre>
</li>
<li>
<p><strong>C</strong></p>

<pre><code>#include &lt;fcntl.h&gt;
#include &lt;stdio.h&gt;
#include &lt;sys/mman.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;sys/types.h&gt;
int main(void)
{
    int fd;
    float *left, *right, *disp;
    fd = open("../left.bin", O_RDONLY);
    left = mmap(NULL, 1 * 70 * 370 * 1226 * sizeof(float), PROT_READ, MAP_SHARED, fd, 0);
    close(fd);
    fd = open("../right.bin", O_RDONLY);
    right = mmap(NULL, 1 * 70 * 370 * 1226 * sizeof(float), PROT_READ, MAP_SHARED, fd, 0);
    close(fd);
    fd = open("../disp.bin", O_RDONLY);
    disp = mmap(NULL, 1 * 1 * 370 * 1226 * sizeof(float), PROT_READ, MAP_SHARED, fd, 0);
    close(fd);
    return 0;
}
</code></pre>
</li>
</ul>

<h2>
<a id="train" class="anchor" href="#train" aria-hidden="true"><span class="octicon octicon-link"></span></a>Train</h2>

<p>This section explains how to train the convolutional neural network on the
KITTI and Middlebury data sets.</p>

<h3>
<a id="kitti" class="anchor" href="#kitti" aria-hidden="true"><span class="octicon octicon-link"></span></a>KITTI</h3>

<p>Download both</p>

<ul>
<li>the <a href="http://www.cvlibs.net/download.php?file=data_stereo_flow.zip">KITTI 2012</a> data set and unzip it
into <code>data.kitti/unzip</code> (you should end up with a file <code>data.kitti/unzip/training/image_0/000000_10.png</code>) and </li>
<li>the <a href="http://www.cvlibs.net/download.php?file=data_scene_flow.zip">KITTI 2015</a> data set and unzip it
into <code>data.kitti2015/unzip</code> (you should end up with a file <code>data.kitti2015/unzip/training/image_2/000000_10.png</code>).</li>
</ul>

<p>Run the preprocessing script:</p>

<pre><code>$ ./preprocess_kitti.lua
dataset 2012
1
...
389
dataset 2015
1
...
400
</code></pre>

<p>Run <code>main.lua</code> to train the network:</p>

<pre><code>$ ./main.lua kitti slow -a train_tr
kitti slow -a train_tr 
conv(in=1, out=112, k=3)
cudnn.ReLU
conv(in=112, out=112, k=3)
cudnn.ReLU
conv(in=112, out=112, k=3)
cudnn.ReLU
conv(in=112, out=112, k=3)
cudnn.ReLU
nn.Reshape(128x224)
nn.Linear(224 -&gt; 384)
cudnn.ReLU
nn.Linear(384 -&gt; 384)
cudnn.ReLU
nn.Linear(384 -&gt; 384)
cudnn.ReLU
nn.Linear(384 -&gt; 384)
cudnn.ReLU
nn.Linear(384 -&gt; 1)
cudnn.Sigmoid
...
</code></pre>

<p>The network is trained on a subset of all training examples with the remaining
examples used for validation; to train on all examples replace use</p>

<pre><code>$ ./main.lua kitti slow -a train_all
</code></pre>

<p>In the previous command, the KITTI 2012 data set is used. If you wish to train
on the KITTI 2015 run</p>

<pre><code>$ ./main.lua kitti2015 slow -a train_tr
</code></pre>

<p>To train the fast architecture instead of the accurate architecture use</p>

<pre><code>$ ./main.lua kitti fast -a train_tr
</code></pre>

<p>The network is stored in the <code>net/</code> directory.</p>

<pre><code>$ ls net/
...
net_kitti2012_fast_-action_train_tr.t7
...
</code></pre>

<h3>
<a id="middlebury" class="anchor" href="#middlebury" aria-hidden="true"><span class="octicon octicon-link"></span></a>Middlebury</h3>

<p>Run <code>download_middlebury.sh</code> to download the training data
(this can take a long time, depending on your internet connection).</p>

<pre><code>$ ./download_middlebury.sh
</code></pre>

<p>The data set is downloaded to the <code>data.mb/unzip</code> directory.</p>

<p>Compile the <a href="http://vision.middlebury.edu/stereo/submit3/">MiddEval3-SDK</a>. You
should end up with the <code>computemask</code> binary in one of the directories listed in
your <code>PATH</code> enviromential variable.  </p>

<p>Install <a href="http://www.imagemagick.org/script/index.php">ImageMagick</a>, as the
preprocessing steps requires the <code>convert</code> binary to resize the images.</p>

<p>Run the preprocessing script:</p>

<pre><code>$ mkdir data.mb.imperfect_gray
$ ./preprocess_mb.py imperfect gray
Adirondack
Backpack
...
testH/Staircase
</code></pre>

<p>The preprocessing is slow (it takes around 30 minutes) the first time it is
run, because the images have to be resized.</p>

<p>Use <code>main.lua</code> to train the network:</p>

<pre><code>$ ./main.lua mb slow -a train_tr
</code></pre>

<h2>
<a id="other-useful-commands" class="anchor" href="#other-useful-commands" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other Useful Commands</h2>

<p>Compute the loss on the validation set:</p>

<pre><code>$ ./main.lua kitti fast -a test_te -net_fname net/net_kitti_fast_-a_train_tr.t7 
kitti fast -a test_te -net_fname net/net_kitti_fast_-a_train_tr.t7 
0.86836290359497        0.0082842716717202
...
0.73244595527649        0.024202708004929
0.72730183601379        0.023603160822285
0.030291934952454
</code></pre>

<p>The validation error rate of the fast architecture on the KITTI 2012 data set is 3.02%.</p>

<p>***</p>

<p>Prepare files for submission to the KITTI and Middlebury evaluation server.</p>

<pre><code>$ ./main.lua kitti fast -a submit -net_fname net/net_kitti_fast_-a_train_tr.t7 
kitti fast -a submit -net_fname net/net_kitti_fast_-a_train_tr.t7 
  adding: 000038_10.png (deflated 0%)
  adding: 000124_10.png (deflated 0%)
  ...
  adding: 000021_10.png (deflated 0%)
</code></pre>

<p>The output is stored in <code>out/submission.zip</code> and can be used to submit to the
<a href="http://www.cvlibs.net/datasets/kitti/user_submit.php">KITTI evaluation
server</a>.</p>

<p>***</p>

<p>Experiment with different network architectures:</p>

<pre><code>$ ./main.lua kitti slow -a train_tr -l1 2 -fm 128 -l2 3 -nh2 512
kitti slow -a train_tr -l1 2 -fm 128 -l2 3 -nh2 512 
conv(in=1, out=128, k=3)
cudnn.ReLU
conv(in=128, out=128, k=3)
cudnn.ReLU
nn.Reshape(128x256)
nn.Linear(256 -&gt; 512)
cudnn.ReLU
nn.Linear(512 -&gt; 512)
cudnn.ReLU
nn.Linear(512 -&gt; 512)
cudnn.ReLU
nn.Linear(512 -&gt; 1)
cudnn.Sigmoid
...
</code></pre>

<p>***</p>

<p>Measure the runtime on a particular data set:</p>

<pre><code>$ ./main.lua kitti fast -a time
kitti fast -a time 
conv(in=1, out=64, k=3)
cudnn.ReLU
conv(in=64, out=64, k=3)
cudnn.ReLU
conv(in=64, out=64, k=3)
cudnn.ReLU
conv(in=64, out=64, k=3)
nn.Normalize2
nn.StereoJoin1
0.73469495773315
</code></pre>

<p>It take 0.73 seconds to run the fast architecure on the KITTI 2012 data set. If
you care only about the time spent in the neural network, you can terminate the
stereo method early:</p>

<pre><code>$ ./main.lua kitti fast -a time -sm_terminate cnn
kitti fast -a time -sm_terminate cnn 
conv(in=1, out=64, k=3)
cudnn.ReLU
conv(in=64, out=64, k=3)
cudnn.ReLU
conv(in=64, out=64, k=3)
cudnn.ReLU
conv(in=64, out=64, k=3)
nn.Normalize2
nn.StereoJoin1
0.31126594543457
</code></pre>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/jzbontar/mc-cnn">mc-cnn</a> is maintained by <a href="https://github.com/jzbontar">jzbontar</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
