{"name":"mc-cnn","tagline":"Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches","body":"Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches\r\n===================================================================================\r\n\r\nThe repository contains\r\n\r\n- procedures to compute the stereo matching cost with a convolutional neural network;\r\n- procedures to train a convolutional neural network on the stereo matching task;\r\n- a basic stereo method (cross-based cost aggregation, semiglobal matching,\r\n  left-right consistency check, median filter, and bilateral filter); and\r\n\r\nA NVIDIA GPU with at least 6 GB of memory is required to run on the KITTI\r\ndata set and 12 GB to run on the Middlebury data set. We tested the code on GTX\r\nTitan (KITTI only), K80, and GTX Titan X.  Note that the network architecture\r\ndeviates from the description in the [CVPR\r\npaper](http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Zbontar_Computing_the_Stereo_2015_CVPR_paper.html);\r\nwe describe the differences in our upcoming journal paper.  Please cite the\r\n[CVPR\r\npaper](http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Zbontar_Computing_the_Stereo_2015_CVPR_paper.html)\r\nif you use code from this repository in your work.\r\n\r\n\t@InProceedings{Zbontar_2015_CVPR,\r\n\t\tauthor = {Zbontar, Jure and LeCun, Yann},\r\n\t\ttitle = {Computing the Stereo Matching Cost With a Convolutional Neural Network},\r\n\t\tjournal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\r\n\t\tmonth = {June},\r\n\t\tyear = {2015}\r\n\t}\r\n\r\nThe code is released under the BSD 2-Clause license.\r\n\r\n\r\nDownload trained networks\r\n-------------------------\r\n\r\n- [KITTI 2012 fast](https://s3.amazonaws.com/mc-cnn/net_kitti_fast_-a_train_all.t7)\r\n- [KITTI 2012 accurate](https://s3.amazonaws.com/mc-cnn/net_kitti_slow_-a_train_all.t7)\r\n- [KITTI 2015 fast](https://s3.amazonaws.com/mc-cnn/net_kitti2015_fast_-a_train_all.t7)\r\n- [KITTI 2015 accurate](https://s3.amazonaws.com/mc-cnn/net_kitti2015_slow_-a_train_all.t7)\r\n- [Middlebury fast](https://s3.amazonaws.com/mc-cnn/net_mb_fast_-a_train_all.t7)\r\n- [Middlebury accurate](https://s3.amazonaws.com/mc-cnn/net_mb_slow_-a_train_all.t7)\r\n\r\nCompute the Matching Cost\r\n-------------------------\r\n\r\nInstall [Torch](http://torch.ch/), [OpenCV 2.4](http://opencv.org/), and\r\n[png++](http://www.nongnu.org/pngpp/).\r\n\r\nRun all following commands in the same directory as this README file.\r\n\r\nCompile the shared libraries:\r\n\r\n\t$ cp Makefile.proto Makefile\r\n\t$ make\r\n\r\nThe command should produce two files: `libadcensus.so` and `libcv.so`.\r\n\r\nTo run the stereo algorithm on a stereo pair from the KITTI 2012 training set&mdash;\r\n\r\n- Left input image  \r\n  <img src=\"samples/input/kittiL.png\" style=\"width: 25%;\"/>\r\n- Right input image  \r\n  <img src=\"samples/input/kittiR.png\" style=\"width: 25%;\"/>\r\n\r\n&mdash;call `main.lua` with the following arguments:\r\n\r\n\t$ ./main.lua kitti fast -a predict -net_fname net/net_kitti_fast_-a_train_tr.t7 -left samples/input/kittiL.png -right samples/input/kittiR.png -disp_max 70\r\n\tWriting right.bin, 1 x 70 x 370 x 1226\r\n\tWriting left.bin, 1 x 70 x 370 x 1226\r\n\tWriting disp.bin, 1 x 1 x 370 x 1226\r\n\r\nThe first two arguments (`kitti fast`) are used to set the default\r\nhyperparameters of the stereo method. The outputs are stored as three binary\r\nfiles:\r\n\r\n- `left.bin`: The matching cost after semiglobal matching and cross-based\r\n  cost aggregation with the left image treated as the reference image.  \r\n  <img src=\"samples/output/left.png\" style=\"width: 25%;\"/>\r\n- `right.bin`: Same as `left.bin`, but with the right image treated as the\r\n  reference image.  \r\n  <img src=\"samples/output/right.png\" style=\"width: 25%;\"/>\r\n- `disp.bin`: The disparity map after the full stereo method.  \r\n  <img src=\"samples/output/disp.png\" style=\"width: 25%;\"/>\r\n\r\nUse the `bin2png.lua` script to generate the `.png` images like the ones above:\r\n\r\n\t$ luajit samples/bin2png.lua \r\n\tWriting left.png\r\n\tWriting right.png\r\n\tWriting disp.png\r\n\r\nIf you wish to use the raw convolutional neural network outputs, that is,\r\nwithout applying cross-based cost aggregation and semiglobal matching, run\r\nthe following command:\r\n\r\n\t$ ./main.lua kitti fast -a predict -net_fname net/net_kitti_fast_-a_train_tr.t7 -left samples/input/kittiL.png -right samples/input/kittiR.png -disp_max 70 -sm_terminate cnn\r\n\tWriting right.bin, 1 x 70 x 370 x 1226\r\n\tWriting left.bin, 1 x 70 x 370 x 1226\r\n\tWriting disp.bin, 1 x 1 x 370 x 1226\r\n\r\nThe resulting disparity maps should look like this:\r\n\r\n- `left.png`  \r\n  <img src=\"samples/output/left_cnn.png\" style=\"width: 25%;\"/>\r\n- `right.png`  \r\n  <img src=\"samples/output/right_cnn.png\" style=\"width: 25%;\"/>\r\n\r\n### Load the Output Binary Files ###\r\n\r\nYou load the binary files by memory mapping them.  We include examples of\r\nmemory mapping for some of the more popular programming languages.\r\n\r\n- **Lua**\r\n\r\n\t\trequire 'torch'\r\n\t\tleft = torch.FloatTensor(torch.FloatStorage('../left.bin')):view(1, 70, 370, 1226)\r\n\t\tright = torch.FloatTensor(torch.FloatStorage('../right.bin')):view(1, 70, 370, 1226)\r\n\t\tdisp = torch.FloatTensor(torch.FloatStorage('../disp.bin')):view(1, 1, 370, 1226)\r\n\r\n- **Python**\r\n\r\n\t\timport numpy as np\r\n\t\tleft = np.memmap('../left.bin', dtype=np.float32, shape=(1, 70, 370, 1226))\r\n\t\tright = np.memmap('../right.bin', dtype=np.float32, shape=(1, 70, 370, 1226))\r\n\t\tdisp = np.memmap('../disp.bin', dtype=np.float32, shape=(1, 1, 370, 1226))\r\n\r\n- **Matlab**\r\n\r\n\t\tleft = memmapfile('../left.bin', 'Format', 'single').Data;\r\n\t\tleft = reshape(left, [1 70 370 1226]);\r\n\t\tright = memmapfile('../right.bin', 'Format', 'single').Data;\r\n\t\tright = reshape(right, [1 70 370 1226]);\r\n\t\tdisp = memmapfile('../disp.bin', 'Format', 'single').Data;\r\n\t\tdisp = reshape(right, [1 1 370 1226]);\r\n\r\n- **C**\r\n\r\n\t\t#include <fcntl.h>\r\n\t\t#include <stdio.h>\r\n\t\t#include <sys/mman.h>\r\n\t\t#include <sys/stat.h>\r\n\t\t#include <sys/types.h>\r\n\t\tint main(void)\r\n\t\t{\r\n\t\t\tint fd;\r\n\t\t\tfloat *left, *right, *disp;\r\n\t\t\tfd = open(\"../left.bin\", O_RDONLY);\r\n\t\t\tleft = mmap(NULL, 1 * 70 * 370 * 1226 * sizeof(float), PROT_READ, MAP_SHARED, fd, 0);\r\n\t\t\tclose(fd);\r\n\t\t\tfd = open(\"../right.bin\", O_RDONLY);\r\n\t\t\tright = mmap(NULL, 1 * 70 * 370 * 1226 * sizeof(float), PROT_READ, MAP_SHARED, fd, 0);\r\n\t\t\tclose(fd);\r\n\t\t\tfd = open(\"../disp.bin\", O_RDONLY);\r\n\t\t\tdisp = mmap(NULL, 1 * 1 * 370 * 1226 * sizeof(float), PROT_READ, MAP_SHARED, fd, 0);\r\n\t\t\tclose(fd);\r\n\t\t\treturn 0;\r\n\t\t}\r\n\r\nTrain\r\n-----\r\n\r\nThis section explains how to train the convolutional neural network on the\r\nKITTI and Middlebury data sets.\r\n\r\n### KITTI ###\r\n\r\nDownload both\r\n\r\n- the [KITTI 2012](http://www.cvlibs.net/download.php?file=data_stereo_flow.zip) data set and unzip it\r\ninto `data.kitti/unzip` (you should end up with a file `data.kitti/unzip/training/image_0/000000_10.png`) and \r\n- the [KITTI 2015](http://www.cvlibs.net/download.php?file=data_scene_flow.zip) data set and unzip it\r\ninto `data.kitti2015/unzip` (you should end up with a file `data.kitti2015/unzip/training/image_2/000000_10.png`).\r\n\r\nRun the preprocessing script:\r\n\r\n\t$ ./preprocess_kitti.lua\r\n\tdataset 2012\r\n\t1\r\n\t...\r\n\t389\r\n\tdataset 2015\r\n\t1\r\n\t...\r\n\t400\r\n\r\nRun `main.lua` to train the network:\r\n\r\n\t$ ./main.lua kitti slow -a train_tr\r\n\tkitti slow -a train_tr \r\n\tconv(in=1, out=112, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=112, out=112, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=112, out=112, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=112, out=112, k=3)\r\n\tcudnn.ReLU\r\n\tnn.Reshape(128x224)\r\n\tnn.Linear(224 -> 384)\r\n\tcudnn.ReLU\r\n\tnn.Linear(384 -> 384)\r\n\tcudnn.ReLU\r\n\tnn.Linear(384 -> 384)\r\n\tcudnn.ReLU\r\n\tnn.Linear(384 -> 384)\r\n\tcudnn.ReLU\r\n\tnn.Linear(384 -> 1)\r\n\tcudnn.Sigmoid\r\n\t...\r\n\r\nThe network is trained on a subset of all training examples with the remaining\r\nexamples used for validation; to train on all examples replace use\r\n\r\n\t$ ./main.lua kitti slow -a train_all\r\n\r\nIn the previous command, the KITTI 2012 data set is used. If you wish to train\r\non the KITTI 2015 run\r\n\r\n\t$ ./main.lua kitti2015 slow -a train_tr\r\n\r\nTo train the fast architecture instead of the accurate architecture use\r\n\r\n\t$ ./main.lua kitti fast -a train_tr\r\n\r\nThe network is stored in the `net/` directory.\r\n\r\n\t$ ls net/\r\n\t...\r\n\tnet_kitti2012_fast_-action_train_tr.t7\r\n\t...\r\n\r\n### Middlebury ###\r\n\r\nRun `download_middlebury.sh` to download the training data\r\n(this can take a long time, depending on your internet connection).\r\n\r\n\t$ ./download_middlebury.sh\r\n\r\nThe data set is downloaded to the `data.mb/unzip` directory.\r\n\r\nCompile the [MiddEval3-SDK](http://vision.middlebury.edu/stereo/submit3/). You\r\nshould end up with the `computemask` binary in one of the directories listed in\r\nyour `PATH` enviromential variable.  \r\n\r\nInstall [ImageMagick](http://www.imagemagick.org/script/index.php), as the\r\npreprocessing steps requires the `convert` binary to resize the images.\r\n\r\nRun the preprocessing script:\r\n\r\n\t$ mkdir data.mb.imperfect_gray\r\n\t$ ./preprocess_mb.py imperfect gray\r\n\tAdirondack\r\n\tBackpack\r\n\t...\r\n\ttestH/Staircase\r\n\r\nThe preprocessing is slow (it takes around 30 minutes) the first time it is\r\nrun, because the images have to be resized.\r\n\r\nUse `main.lua` to train the network:\r\n\r\n\t$ ./main.lua mb slow -a train_tr\r\n\r\nOther Useful Commands\r\n---------------------\r\n\r\nCompute the loss on the validation set:\r\n\r\n\t$ ./main.lua kitti fast -a test_te -net_fname net/net_kitti_fast_-a_train_tr.t7 \r\n\tkitti fast -a test_te -net_fname net/net_kitti_fast_-a_train_tr.t7 \r\n\t0.86836290359497        0.0082842716717202\r\n\t...\r\n\t0.73244595527649        0.024202708004929\r\n\t0.72730183601379        0.023603160822285\r\n\t0.030291934952454\r\n\r\nThe validation error rate of the fast architecture on the KITTI 2012 data set is 3.02%.\r\n\r\n\\***\r\n\r\nPrepare files for submission to the KITTI and Middlebury evaluation server.\r\n\r\n\t$ ./main.lua kitti fast -a submit -net_fname net/net_kitti_fast_-a_train_tr.t7 \r\n\tkitti fast -a submit -net_fname net/net_kitti_fast_-a_train_tr.t7 \r\n\t  adding: 000038_10.png (deflated 0%)\r\n\t  adding: 000124_10.png (deflated 0%)\r\n\t  ...\r\n\t  adding: 000021_10.png (deflated 0%)\r\n\r\nThe output is stored in `out/submission.zip` and can be used to submit to the\r\n[KITTI evaluation\r\nserver](http://www.cvlibs.net/datasets/kitti/user_submit.php).\r\n\r\n\\***\r\n\r\nExperiment with different network architectures:\r\n\r\n\t$ ./main.lua kitti slow -a train_tr -l1 2 -fm 128 -l2 3 -nh2 512\r\n\tkitti slow -a train_tr -l1 2 -fm 128 -l2 3 -nh2 512 \r\n\tconv(in=1, out=128, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=128, out=128, k=3)\r\n\tcudnn.ReLU\r\n\tnn.Reshape(128x256)\r\n\tnn.Linear(256 -> 512)\r\n\tcudnn.ReLU\r\n\tnn.Linear(512 -> 512)\r\n\tcudnn.ReLU\r\n\tnn.Linear(512 -> 512)\r\n\tcudnn.ReLU\r\n\tnn.Linear(512 -> 1)\r\n\tcudnn.Sigmoid\r\n\t...\r\n\r\n\\***\r\n\r\nMeasure the runtime on a particular data set:\r\n\r\n\t$ ./main.lua kitti fast -a time\r\n\tkitti fast -a time \r\n\tconv(in=1, out=64, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=64, out=64, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=64, out=64, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=64, out=64, k=3)\r\n\tnn.Normalize2\r\n\tnn.StereoJoin1\r\n\t0.73469495773315\r\n\r\nIt take 0.73 seconds to run the fast architecure on the KITTI 2012 data set. If\r\nyou care only about the time spent in the neural network, you can terminate the\r\nstereo method early:\r\n\r\n\t$ ./main.lua kitti fast -a time -sm_terminate cnn\r\n\tkitti fast -a time -sm_terminate cnn \r\n\tconv(in=1, out=64, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=64, out=64, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=64, out=64, k=3)\r\n\tcudnn.ReLU\r\n\tconv(in=64, out=64, k=3)\r\n\tnn.Normalize2\r\n\tnn.StereoJoin1\r\n\t0.31126594543457\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}